<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Hidden Markov Models &#8212; deeptime 0.4.5 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=822347c1" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=1e544560" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8ffeb421" />
    <link rel="stylesheet" type="text/css" href="../_static/perfect-scrollbar/css/perfect-scrollbar.css?v=9f1a41f4" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
    <script src="../_static/documentation_options.js?v=340bb9e5"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/perfect-scrollbar/js/perfect-scrollbar.min.js?v=a8314474"></script>
    <script src="../_static/d3.v5.min.js?v=5b739c40"></script>
    <script src="../_static/d3-legend.min.js?v=5895a191"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Clustering" href="clustering.html" />
    <link rel="prev" title="TRAM" href="tram.html" />
<link rel="apple-touch-icon" sizes="180x180" href="../_static/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../_static/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../_static/favicon-16x16.png">
<link rel="manifest" href="../_static/site.webmanifest">

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="../index.html">
    <img class="logo" src="../_static/logo/deeptime_romand_white.svg" alt="Logo"/>
    
  </a>
</p>










<!-- h3>Navigation</h3 -->
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index_dimreduction.html">Dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_deepdimreduction.html">Deep dim reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="sindy.html">SINDy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_msm.html">Markov state models</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Hidden Markov Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Estimation">Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Bayesian-sampling">Bayesian sampling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_nbexamples.html">Notebook examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index_dev.html">For developers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/index_base.html">deeptime.base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_clustering.html">deeptime.clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_sindy.html">deeptime.sindy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_covariance.html">deeptime.covariance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_decomposition.html">deeptime.decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_markov.html">deeptime.markov</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_markov_hmm.html">deeptime.markov.hmm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_markov_tools.html">deeptime.markov.tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_basis.html">deeptime.basis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_kernels.html">deeptime.kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_data.html">deeptime.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_plots.html">deeptime.plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_numeric.html">deeptime.numeric</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index_util.html">deeptime.util</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../imprint.html">Imprint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">Software License</a></li>
</ul>





<!--
<p>
    <iframe src="https://ghbtns.com/github-btn.html?user=deeptime-ml&repo=deeptime&type=star&count=true&size=large&v=2"
            allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>
--><p class="caption">
    <span class="caption-text">Version</span>
</p>
<ul>
    <li class="toctree-l1">
        <a class="reference internal" id="version-latest"
           href="../latest/../index.html">Latest release
            (0.4.5)</a>
    </li>
    <li class="toctree-l1">
        <a class="reference internal" id="version-trunk"
           href="../trunk/../index.html">Trunk version</a>
    </li>
</ul>
<script>
    if (window.location.href.indexOf("latest") > -1) {
        document.getElementById("version-latest").className = "reference internal current";
        document.getElementById("version-trunk").className = "reference internal";
    } else {
        document.getElementById("version-latest").className = "reference internal";
        document.getElementById("version-trunk").className = "reference internal current";
    }
</script><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../contents.html">Documentation overview</a><ul>
      <li>Previous: <a href="tram.html" title="previous chapter">TRAM</a></li>
      <li>Next: <a href="clustering.html" title="next chapter">Clustering</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" placeholder="Quick search" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script><div class="github-wrapper">
    <a href="https://github.com/deeptime-ml/deeptime"
       style="width: 16rem; height:100%; position:absolute; top:0; left:0;">
        <img class="side-logo logo-github" src="../_static/GitHub-Mark-Light-32px.png" alt="gh"/>
        <div style="display: table; height: 100%; overflow: hidden;">
            <div style="display: table-cell; vertical-align: middle;">
                <div>deeptime &#64; GitHub
                </div>
            </div>
        </div>
    </a>
</div>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="Hidden-Markov-Models">
<h1>Hidden Markov Models<a class="headerlink" href="#Hidden-Markov-Models" title="Link to this heading">Â¶</a></h1>
<p>For users already familiar with the interface, the <a class="reference internal" href="../api/index_markov_hmm.html"><span class="doc">API docs</span></a>.</p>
<p>Hidden Markov models (HMM) are a type of Markov model where the underlying Markov process <span class="math">\(X_t\)</span> is <em>hidden</em> and there is an <em>observable</em> process <span class="math">\(Y_t\)</span> which depends on <span class="math">\(X_t\)</span>. A nice introduction into HMM theory and related algorithms can be found in <a class="footnote-reference brackets" href="#footcite-rabiner1989tutorial" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. When fitting such a model, the goal is to parameterize a Markov model over <span class="math">\(X_t\)</span> by observing <span class="math">\(Y_t\)</span>. Here the dependency of <span class="math">\(Y_t\)</span> on <span class="math">\(X_t\)</span> must only depend on the current time
step, i.e.,</p>
<div class="math">
\[\mathbb{P}(Y_t \in A \mid X_1 = x_1, \ldots, X_n = x_n) = \mathbb{P}(Y_t \in A \mid X_n = x_n),

\]</div>
<p>where <span class="math">\(A\)</span> is some measurable set in observation space.</p>
<p>The states of process <span class="math">\(X_i\)</span> are often called âhidden statesâ or âmacro statesâ, whereas <span class="math">\(\mathbb{P}(Y_t\in A \mid X_n = x_n)\)</span> is referred to as output probability or emission probability. If the emission probability is a discrete probability distribution, the corresponding states are often called âobservableâ or âmicroâ states.</p>
<p>In particular, this means that here - as opposed to in ordinary <a class="reference internal" href="mlmsm.html"><span class="doc">Markov state models</span></a> - one hidden state maps to a probability distribution over the observable space in which <span class="math">\(Y_t\)</span> takes its values.</p>
<p>HMMs are generally more expressive as MSMs, as each MSM can be written in terms of an HMM with discrete output probabilities: The transition matrix can be used as hidden transition matrix and the emission probabilities reduce to the identity, i.e., there are as many hidden states as there are observable states and each hidden state maps to a delta distribution. HMMs relax this by allowing for other distributions too.</p>
<p>Due to their more general nature, HMMs can recover from situations and produce good results when MSMs would not, for example in some cases of bad state space discretization. However, HMMs are harder to fit, computationally more expensive, and the estimation likes to get stuck in local optima.</p>
<p>In deeptime, a hidden Markov model is determined by</p>
<ul class="simple">
<li><p>a hidden transition model <span class="math">\(P\in\mathbb{R}^{n\times n}\)</span> (of type <a class="reference internal" href="../api/generated/deeptime.markov.msm.MarkovStateModel.html#deeptime.markov.msm.MarkovStateModel"><span class="std std-ref">MarkovStateModel</span></a>) describing the hidden process,</p></li>
<li><p>an <a class="reference internal" href="../api/generated/deeptime.markov.hmm.OutputModel.html#deeptime.markov.hmm.OutputModel"><span class="std std-ref">output model</span></a>, which holds information on the emission probabilities,</p></li>
<li><p>and an initial distribution <span class="math">\(\pi\in\mathbb{R}^n\)</span> over the hidden states, which expresses a prior belief over the average most likely distribution of hidden states.</p></li>
</ul>
<p>Creating an <a class="reference internal" href="../api/generated/deeptime.markov.hmm.HiddenMarkovModel.html#deeptime.markov.hmm.HiddenMarkovModel"><span class="std std-ref">HMM</span></a> is as simple as importing deeptime and fixing at least the transition model and the output model.</p>
<p>Here <span class="math">\(P\in\mathbb{R}^{2\times 2}\)</span> is the hidden transition matrix and <span class="math">\(E\in\mathbb{R}^{2\times 3}\)</span> are the emission probabilities mapping from two hidden states to two three-state output probability distributions:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">deeptime.markov.hmm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hmm</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]])</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">ground_truth</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">HiddenMarkovModel</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">E</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This hidden Markov model has two hidden (macro) states and three output (micro) states. The initial distribution defaults to a uniform distribution, which prefers no hidden state over the other.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# hidden states:&quot;</span><span class="p">,</span> <span class="n">ground_truth</span><span class="o">.</span><span class="n">n_hidden_states</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# observation states:&quot;</span><span class="p">,</span> <span class="n">ground_truth</span><span class="o">.</span><span class="n">n_observation_states</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# hidden states: 2
# observation states: 3
</pre></div></div>
</div>
<p>We can now generate an hidden and corresponding observation sequence from the hmm:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">hidden_trajectory</span><span class="p">,</span> <span class="n">observation_trajectory</span><span class="p">)</span> <span class="o">=</span> <span class="n">ground_truth</span><span class="o">.</span><span class="n">simulate</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Visualizing the hidden trajectory, one can observe that state 0 is preferred. The observation trajectory jumps between three states. If the hidden state is <span class="math">\(0\)</span>, the observation trajectory is most of the time in micro state <span class="math">\(2\)</span>, otherwise it is in state <span class="math">\(0\)</span> or <span class="math">\(1\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">constrained_layout</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_gridspec</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">0</span><span class="p">])</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hidden_trajectory</span><span class="p">[:</span><span class="mi">200</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;macrostate&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">observation_trajectory</span><span class="p">[:</span><span class="mi">200</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;microstate&quot;</span><span class="p">)</span>

<span class="n">probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i_hidden</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i_obs</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="c1"># indices where macro state is i_hidden</span>
        <span class="n">hidden_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">hidden_trajectory</span> <span class="o">==</span> <span class="n">i_hidden</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">subselected_obs</span> <span class="o">=</span> <span class="n">observation_trajectory</span><span class="p">[</span><span class="n">hidden_ix</span><span class="p">]</span>
        <span class="c1"># indices where micro state is i_obs given i_hidden</span>
        <span class="n">obs_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">subselected_obs</span> <span class="o">==</span> <span class="n">i_obs</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">probabilities</span><span class="p">[</span><span class="n">i_hidden</span><span class="p">,</span> <span class="n">i_obs</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs_ix</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_ix</span><span class="p">)</span>

<span class="n">cb</span> <span class="o">=</span> <span class="n">ax3</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Reds&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks_position</span><span class="p">(</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;micro state&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;macro state&quot;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;conditional probabilities $P(Y_t\in s_\mathrm</span><span class="si">{micro}</span><span class="s2">\mid X_t\in s_\mathrm</span><span class="si">{macro}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cb</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_hmm_7_0.png" src="../_images/notebooks_hmm_7_0.png" />
</div>
</div>
<p>While the output probabilities in this case map to discrete values, this is not the only option. We implement</p>
<ul class="simple">
<li><p>the <a class="reference internal" href="../api/generated/deeptime.markov.hmm.DiscreteOutputModel.html#deeptime.markov.hmm.DiscreteOutputModel"><span class="std std-ref">discrete output model</span></a>, which maps to a discrete observable space and is parameterized by a row-stochastic output probability matrix <span class="math">\(E\in\mathbb{R}^{n\times m}\)</span>, where <span class="math">\(n\)</span> is the number hidden and <span class="math">\(m\)</span> the number of observable states,</p></li>
<li><p>and the <a class="reference internal" href="../api/generated/deeptime.markov.hmm.GaussianOutputModel.html#deeptime.markov.hmm.GaussianOutputModel"><span class="std std-ref">Gaussian output model</span></a>, which maps to a one-dimensional output space with <span class="math">\(m\)</span> Gaussians, each parameterized by mean and standard deviation.</p></li>
</ul>
<section id="Estimation">
<h2>Estimation<a class="headerlink" href="#Estimation" title="Link to this heading">Â¶</a></h2>
<p>The estimation of an HMM given sequences of observations can be performed using the Baum-Welch algorithm <a class="footnote-reference brackets" href="#footcite-baum1967inequality" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>, which belongs to the family of expectation-maximization algorithms.</p>
<p>It crucially requires an initial guess for emission probabilities and hidden state transition matrix, as the optimization is prone to getting stuck in local optima. After optimization, the likelihood for a trained model is available and can be compared to models with different initial guesses.</p>
<p>Going back to above example, we can try to fit an HMM based on the observation trajectory for different initial guesses:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hmm_est_real</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MaximumLikelihoodHMM</span><span class="p">(</span>
    <span class="n">lagtime</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_model</span><span class="o">=</span><span class="n">ground_truth</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">observation_trajectory</span><span class="p">)</span><span class="o">.</span><span class="n">fetch_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum absolute deviation: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hmm_est_real</span><span class="o">.</span><span class="n">transition_model</span><span class="o">.</span><span class="n">transition_matrix</span> <span class="o">-</span> <span class="n">P</span><span class="p">))</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Maximum absolute deviation: 0.034
</pre></div></div>
</div>
<p>When initialized with the ground truth, the deviation is not large from the initial model. Deeptime furthermore offers some initialization heuristics should the ground truth be unknown.</p>
<p>For discrete output models:</p>
<ol class="arabic">
<li><p><a class="reference internal" href="../api/generated/deeptime.markov.hmm.init.discrete.metastable_from_data.html#deeptime.markov.hmm.init.discrete.metastable_from_data"><span class="std std-ref">init.discrete.metastable_from_data</span></a> or <a class="reference internal" href="../api/generated/deeptime.markov.hmm.init.discrete.metastable_from_msm.html#deeptime.markov.hmm.init.discrete.metastable_from_msm"><span class="std std-ref">init.discrete.metastable_from_msm</span></a>: A reversible <a class="reference internal" href="mlmsm.html"><span class="doc">MSM</span></a> is estimated from given discrete trajectories. This is optional in case the MSM is already available. Then, the
estimated transition matrix <span class="math">\(P\in\mathbb{R}^{n\times n}\)</span> is coarse-grained with PCCA+ <a class="footnote-reference brackets" href="#footcite-roblitz2013fuzzy" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, yielding <span class="math">\(m\)</span> metastable/macro states and a membership matrix <span class="math">\(M\in\mathbb{R}^{n\times m}\)</span>. The transition matrix is projected into hidden space via</p>
<div class="math">
\[P_\mathrm{coarse} = (M^\top M)^{-1}M^\top PM.

\]</div>
<p>This procedure is described in greater detail in <a class="footnote-reference brackets" href="#footcite-noe2013projected" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p>
</li>
<li><p><a class="reference internal" href="../api/generated/deeptime.markov.hmm.init.discrete.random_guess.html#deeptime.markov.hmm.init.discrete.random_guess"><span class="std std-ref">init.discrete.random_guess</span></a>: Initializes hidden transition matrix and initial distribution uniformly, the emission probabilities are drawn from the uniform distribution <span class="math">\(\mathcal{U}(0, 1)\)</span> and normalized to form a row-stochastic matrix.</p></li>
</ol>
<p>Letâs create a random initial model and use it as starting point for the maximum-likelihood estimation.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_hmm_random</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">discrete</span><span class="o">.</span><span class="n">random_guess</span><span class="p">(</span><span class="n">n_observation_states</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_hidden_states</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">hmm_est_random</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MaximumLikelihoodHMM</span><span class="p">(</span>
    <span class="n">lagtime</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">initial_model</span><span class="o">=</span><span class="n">init_hmm_random</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">observation_trajectory</span><span class="p">)</span><span class="o">.</span><span class="n">fetch_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Also here, we can compute the maximum deviation from ground truth hidden transition matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum absolute deviation: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">hmm_est_random</span><span class="o">.</span><span class="n">transition_model</span><span class="o">.</span><span class="n">transition_matrix</span> <span class="o">-</span> <span class="n">P</span><span class="p">))</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Maximum absolute deviation: 0.258
</pre></div></div>
</div>
<p>And evaluate the likelihood of the model</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hmm_est_random</span><span class="o">.</span><span class="n">likelihood</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-1652.9913478370843
</pre></div></div>
</div>
<p>As well as compute the likelihood of the generated data given our original model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ground_truth</span><span class="o">.</span><span class="n">compute_observation_likelihood</span><span class="p">(</span><span class="n">observation_trajectory</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-1660.3598109385
</pre></div></div>
</div>
</section>
<section id="Bayesian-sampling">
<h2>Bayesian sampling<a class="headerlink" href="#Bayesian-sampling" title="Link to this heading">Â¶</a></h2>
<p>Bayesian sampling of HMMs can be used to obtain confidences. In particular, Gibbs sampling can be performed around a reference HMM, as introduced in <a class="footnote-reference brackets" href="#footcite-chodera2011bayesian" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>. In deeptime, a <a class="reference internal" href="../api/generated/deeptime.markov.hmm.BayesianHMM.html#deeptime.markov.hmm.BayesianHMM"><span class="std std-ref">BayesianHMM</span></a> estimator is provided.</p>
<p>The easiest method to obtain a BayesianHMM estimator from data is the <a class="reference internal" href="../api/generated/deeptime.markov.hmm.BayesianHMM.html#deeptime.markov.hmm.BayesianHMM.default"><span class="std std-ref">default()</span></a> static method. It estimates an ML-MSM on data, coarse-grains it to the requested number of hidden states and uses it as starting point for an ML-HMM estimation. The resulting model is used as start sample for the BayesianHMM estimator.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">BayesianHMM</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">observation_trajectory</span><span class="p">,</span> <span class="n">n_hidden_states</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">lagtime</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The maximum-likelihood estimation is done for fewer iterations and with less precision than the default settings, as a rough estimate suffices to start sampling.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum absolute deviation: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">initial_hmm</span><span class="o">.</span><span class="n">transition_model</span><span class="o">.</span><span class="n">transition_matrix</span> <span class="o">-</span> <span class="n">P</span><span class="p">))</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Maximum absolute deviation: 0.282
</pre></div></div>
</div>
<p>Calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> on the estimator starts sampling and yields a <a class="reference internal" href="../api/generated/deeptime.markov.hmm.BayesianHMMPosterior.html#deeptime.markov.hmm.BayesianHMMPosterior"><span class="std std-ref">BayesianHMMPosterior</span></a> instance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">observation_trajectory</span><span class="p">,</span>
    <span class="n">n_burn_in</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># number of sampling steps to discard at the beginning</span>
    <span class="n">n_thin</span><span class="o">=</span><span class="mi">10</span>  <span class="c1"># take every 10th sample</span>
<span class="p">)</span><span class="o">.</span><span class="n">fetch_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Any quantity that can be evaluated on the hidden Markov state model object can also be accessed through the posterior but equipped with uncertainties. For this, call <code class="docutils literal notranslate"><span class="pre">gather_stats</span></code> with the name of the method or quantity that should be evaluated. If the method requires arguments, they can also be provided. If it is a method of a lower-level object, it can be separated by slashes <code class="docutils literal notranslate"><span class="pre">/</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">gather_stats</span><span class="p">(</span><span class="s2">&quot;transition_model/transition_matrix&quot;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can check on the confidence of, e.g., the transition matrix:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">R</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{11}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{12}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{21}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{22}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">-</span><span class="mf">.1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_hmm_30_0.png" src="../_images/notebooks_hmm_30_0.png" />
</div>
</div>
<p>Alternatively, it is possible to instantiate a Bayesian HMM estimator directly by using a previously estimated HMM:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">BayesianHMM</span><span class="p">(</span><span class="n">initial_hmm</span><span class="o">=</span><span class="n">posterior</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>And perform sampling as above</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">observation_trajectory</span><span class="p">,</span>
    <span class="n">n_burn_in</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="c1"># number of sampling steps to discard at the beginning</span>
    <span class="n">n_thin</span><span class="o">=</span><span class="mi">10</span>  <span class="c1"># take every 10th sample</span>
<span class="p">)</span><span class="o">.</span><span class="n">fetch_model</span><span class="p">()</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">gather_stats</span><span class="p">(</span><span class="s2">&quot;transition_model/transition_matrix&quot;</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mean</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="o">-</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">R</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{11}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{12}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{21}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="mf">.05</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$P_</span><span class="si">{22}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">-</span><span class="mf">.1</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">-</span><span class="mf">.1</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_hmm_34_0.png" src="../_images/notebooks_hmm_34_0.png" />
</div>
</div>
<p class="rubric">References</p>
<div class="docutils container" id="id6">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-rabiner1989tutorial" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>LawrenceÂ R Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. <em>Proceedings of the IEEE</em>, 77(2):257â286, 1989.</p>
</aside>
<aside class="footnote brackets" id="footcite-baum1967inequality" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>LeonardÂ E Baum and JohnÂ Alonzo Eagon. An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology. <em>Bulletin of the American Mathematical Society</em>, 73(3):360â363, 1967.</p>
</aside>
<aside class="footnote brackets" id="footcite-roblitz2013fuzzy" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Susanna RÃ¶blitz and Marcus Weber. Fuzzy spectral clustering by pcca+: application to markov state models and data classification. <em>Advances in Data Analysis and Classification</em>, 7(2):147â179, 2013.</p>
</aside>
<aside class="footnote brackets" id="footcite-noe2013projected" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Frank NoÃ©, Hao Wu, Jan-Hendrik Prinz, and Nuria Plattner. Projected and hidden markov models for calculating kinetics and metastable states of complex molecules. <em>The Journal of chemical physics</em>, 139(18):11B609_1, 2013.</p>
</aside>
<aside class="footnote brackets" id="footcite-chodera2011bayesian" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>JohnÂ D Chodera, Phillip Elms, Frank NoÃ©, Bettina Keller, ChristianÂ M Kaiser, Aaron Ewall-Wice, Susan Marqusee, Carlos Bustamante, and NinaÂ Singhal Hinrichs. Bayesian hidden markov model analysis of single-molecule force spectroscopy: characterizing kinetics under measurement uncertainty. <em>arXiv preprint arXiv:1108.1430</em>, 2011.</p>
</aside>
</aside>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
<script src="../_static/scrollbar.js"></script>

  </body>
</html>